#!/usr/bin/env bash
#SBATCH --partition=main                  # use main partition
#SBATCH --job-name=moshi_lora        # job name
#SBATCH --output=/home/alufr/ttt_tests/moshi-finetune/logs/inference/moshi_lora.%j.log   # STDOUT ‚Üí logs/inference/moshi_lora.<JOBID>.log
#SBATCH --error=/home/alufr/ttt_tests/moshi-finetune/logs/inference/moshi_lora.%j.err    # STDERR ‚Üí logs/inference/moshi_lora.<JOBID>.err
#SBATCH --ntasks=1                        # one task/process
#SBATCH --cpus-per-task=4                 # CPU cores
#SBATCH --gpus=1                          # 1 GPU
#SBATCH --constraint=rtx_6000             # prefer RTX 6000
#SBATCH --mem=50G                         # host RAM
#SBATCH --time=2:00:00                    # walltime hh:mm:ss (2 hours)
#SBATCH --exclude=cs-6000-01,cs-6000-02,cs-6000-03,cs-6000-04,ise-6000-08,ise-6000-07   # exclude problematic nodes

# Pin to the first (and only) visible GPU
export CUDA_VISIBLE_DEVICES=0

# Fix CUDA memory fragmentation
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Load & activate conda environment
module load anaconda
eval "$(conda shell.bash hook)"
conda activate moshi_ttt_fixed

# Force proper environment activation
export PATH="$CONDA_PREFIX/bin:$PATH"
export LD_LIBRARY_PATH="$CONDA_PREFIX/lib:$LD_LIBRARY_PATH"

# Verify environment
echo "üêç Active conda environment: $CONDA_DEFAULT_ENV"
echo "üêç Python path: $(which python)"
echo "üêç Python version: $(python --version)"

# GPU Diagnostics
echo "üîß GPU Diagnostics:"
echo "üîß Hostname: $(hostname)"
echo "üîß SLURM_JOB_ID: $SLURM_JOB_ID"
echo "üîß CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# Test nvidia-smi
if command -v nvidia-smi &> /dev/null; then
    echo "üîß nvidia-smi output:"
    nvidia-smi || echo "‚ùå nvidia-smi failed"
else
    echo "‚ùå nvidia-smi not found"
fi

# Get parameters from environment variables (set by submit script)
CHECKPOINT_DIR=${CHECKPOINT_DIR}
INPUT_AUDIO=${INPUT_AUDIO}
OUTPUT_AUDIO=${OUTPUT_AUDIO}
HF_REPO=${HF_REPO:-kyutai/moshiko-pytorch-bf16}

# Change to moshi-finetune directory
cd /home/alufr/ttt_tests/moshi-finetune

# Add Moshi to PYTHONPATH
export PYTHONPATH="/home/alufr/ttt_tests/moshi:$PYTHONPATH"

echo "üöÄ Starting Moshi LoRA Inference"
echo "=============================="
echo "üìÅ Checkpoint: $CHECKPOINT_DIR"
echo "üé§ Input audio: $INPUT_AUDIO"
echo "üîä Output audio: $OUTPUT_AUDIO"
echo "ü§ó HF repo: $HF_REPO"
echo ""

# Verify checkpoint exists
if [ ! -d "$CHECKPOINT_DIR" ]; then
    echo "‚ùå Error: Checkpoint directory '$CHECKPOINT_DIR' not found!"
    exit 1
fi

# Verify input audio exists
if [ ! -f "$INPUT_AUDIO" ]; then
    echo "‚ùå Error: Input audio file '$INPUT_AUDIO' not found!"
    exit 1
fi

# Run LoRA inference using standard moshi.run_inference
echo "üèÉ Starting LoRA inference..."
python -m moshi.run_inference \
    --config "$CHECKPOINT_DIR/config.json" \
    --lora-weight "$CHECKPOINT_DIR/lora.safetensors" \
    --hf-repo "$HF_REPO" \
    --device cuda \
    --batch-size 1 \
    "$INPUT_AUDIO" \
    "$OUTPUT_AUDIO"

EXIT_CODE=$?

if [ $EXIT_CODE -eq 0 ]; then
    echo ""
    echo "‚úÖ Inference completed successfully!"
    echo "üîä Output saved to: $OUTPUT_AUDIO"
else
    echo ""
    echo "‚ùå Inference failed with exit code $EXIT_CODE"
    exit $EXIT_CODE
fi
