# Multi-Layer TTT-MLP Trainable Parameters Fix

## Critical Bug Discovered

**Date:** October 9, 2025  
**Severity:** CRITICAL - Multi-layer TTT-MLP parameters were NOT trainable

## Problem Analysis

### Symptom
Training logs showed:
- **Expected trainable parameters:** ~399M (calculated from architecture)
- **Actual trainable parameters:** 336M (reported by system)
- **Missing:** ~63M parameters (exactly 5 layers √ó 12.6M = 63M)

### Root Cause

The multi-layer TTT-MLP implementation (`TTTMLPMultiLayer`) uses `nn.ParameterList` for dynamic layer support:

```python
# In TTTMLPMultiLayer.__init__():
self.weights = nn.ParameterList()  # Creates weights.0, weights.1, weights.2, ...
self.biases = nn.ParameterList()   # Creates biases.0, biases.1, biases.2, ...
```

This creates parameter names like:
- `weights.0`, `weights.1`, `weights.2` (for 3-layer MLP)
- `biases.0`, `biases.1`, `biases.2`

However, `finetune/ttt_utils.py::is_ttt_parameter()` only checked for hardcoded patterns:
- `"w1"`, `"w2"`, `"b1"`, `"b2"` (2-layer patterns)

**Result:** Multi-layer parameters were initialized and used in forward passes, but `requires_grad` was set to `False`, so they were **NOT ACTUALLY TRAINING**.

## Verification

### Parameter Count Math

Per hybrid layer should have:
```python
TTTBase components:
  - Q,K,V,O projections: 67,125,248 params
  - TTT LR gate: 4,128 params
  - TTT norm: 8,192 params
  - Post norm: 8,192 params
  TOTAL TTTBase: 67,145,760 params

TTTMLPMultiLayer (3 layers):
  - Layer 1 (128‚Üí512): 2,113,536 params
  - Layer 2 (512‚Üí512): 8,404,992 params
  - Layer 3 (512‚Üí128): 2,101,248 params
  TOTAL: 12,619,776 params

Gating:
  - forward_ssm_gating: 4,096 params
  - backward_ssm_gating: 4,096 params
  TOTAL: 8,192 params

Per Layer Total: 79,773,728 params
5 Layers: 398,868,640 params
```

**Before fix:** 336,404,640 trainable (missing 62,464,000)  
**Missing amount matches:** 5 layers √ó 12,619,776 = 63,098,880 ‚úì

## The Fix

Added dynamic pattern matching in `finetune/ttt_utils.py`:

```python
ttt_patterns = [
    # ... existing patterns ...
    "w1",                     # TTT MLP first layer (2-layer version)
    "w2",                     # TTT MLP second layer (2-layer version)
    "b1",                     # TTT MLP first bias
    "b2",                     # TTT MLP second bias
    "weights.",               # Multi-layer TTT-MLP weights (weights.0, weights.1, ...)
    "biases.",                # Multi-layer TTT-MLP biases (biases.0, biases.1, ...)
    # ... rest of patterns ...
]
```

The patterns `"weights."` and `"biases."` will match:
- `weights.0`, `weights.1`, `weights.2`, `weights.3`, ... (any number of layers)
- `biases.0`, `biases.1`, `biases.2`, `biases.3`, ... (any number of layers)

## Expected Results After Fix

After this fix and retraining:
- **Trainable parameters:** ~399M (up from 336M)
- **Multi-layer MLP params:** Will have `requires_grad=True`
- **Training:** Will actually update the 3-layer MLP weights
- **Memory:** Slightly higher gradient memory usage expected

## Testing Verification

To verify the fix is working:
1. Check training logs for: `üìä Trainable parameters: ~399M`
2. Verify parameter tracker finds more params (not just 110)
3. Monitor that TTT-MLP gradients are non-zero during training

## Impact

**Before fix:**
- ‚úÖ Multi-layer architecture initialized correctly
- ‚úÖ Forward passes used 3-layer MLP
- ‚ùå Backward passes did NOT update MLP weights
- ‚ùå Training was effectively using frozen random MLP weights

**After fix:**
- ‚úÖ Multi-layer architecture initialized correctly
- ‚úÖ Forward passes use 3-layer MLP
- ‚úÖ Backward passes UPDATE MLP weights
- ‚úÖ Training actually learns multi-layer representations

## Files Modified

1. **finetune/ttt_utils.py** - Added `"weights."` and `"biases."` patterns to `is_ttt_parameter()`

## Next Steps

1. ‚úÖ Fix applied
2. ‚è≥ Submit new training job
3. ‚è≥ Verify trainable param count increases to ~399M
4. ‚è≥ Verify training converges better with trainable multi-layer MLP
