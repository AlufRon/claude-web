# üî• CRITICAL BUG FOUND IN FIGURE 5 LOGGING

## The Bug

**File**: `moshi_ttt/models/ssm/ops/ttt_mlp.py`
**Lines**: 184-196

```python
# For Figure 5: add frozen W‚ÇÄ and position tracking if enabled
if _inner_fig5_enabled and layer_id is not None:
    # Detach frozen initial weights (W‚ÇÄ) for Figure 5 logging
    num_mini_batches = inputs["XK"].shape[0]
    # Weights already have appropriate batch dimensions, just expand them
    inputs["W1_0"] = W1_init.detach().expand(num_mini_batches, -1, -1, -1, -1)  # BUG!
    inputs["b1_0"] = b1_init.detach().expand(num_mini_batches, -1, -1, -1, -1)  # BUG!
    inputs["W2_0"] = W2_init.detach().expand(num_mini_batches, -1, -1, -1, -1)  # BUG!
    inputs["b2_0"] = b2_init.detach().expand(num_mini_batches, -1, -1, -1, -1)  # BUG!
```

## The Problem

**`W1_init` is NOT the frozen initial weights W‚ÇÄ!**

`W1_init` comes from line 157:
```python
W1_init = params_dict["W1_states"].to(torch.float32)
```

And `params_dict["W1_states"]` gets **UPDATED** by `scan()` as it processes mini-batches!

So what we're calling "l0" (loss with frozen W‚ÇÄ) is actually:
- **Position 0**: Uses W‚ÇÄ (correct)
- **Position 1**: Uses W‚ÇÅ (wrong! should use W‚ÇÄ)
- **Position 2**: Uses W‚ÇÇ (wrong! should use W‚ÇÄ)
- **Position t**: Uses W‚Çú (wrong! should use W‚ÇÄ)

**This explains why the "frozen" W‚ÇÄ loss changes over the sequence!**

From `figure5_stats_librilight.json`:
```json
"layer_29": {
  "initial_W0_loss": 1.972,  // Actually l(W‚ÇÄ; x‚ÇÄ) ‚úì
  "final_W0_loss": 2.005,    // Actually l(W‚ÇÇ‚ÇÄ‚ÇÑ‚Çá; x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá) ‚úó Should be l(W‚ÇÄ; x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá)
}
```

## The Fix

We need to **freeze W‚ÇÄ at the very beginning** before entering the scan loop:

```python
def ttt_mlp(...):
    init_params_dict = {
        "W1_states": W1_init,
        "b1_states": b1_init,
        "W2_states": W2_init,
        "b2_states": b2_init,
        ...
    }

    # ‚úÖ FREEZE INITIAL WEIGHTS BEFORE SCAN
    if _inner_fig5_enabled and layer_id is not None:
        # Clone and freeze W‚ÇÄ (these will NEVER change)
        W1_frozen = W1_init.detach().clone()
        b1_frozen = b1_init.detach().clone()
        W2_frozen = W2_init.detach().clone()
        b2_frozen = b2_init.detach().clone()

    inputs = {...}
    inputs = tree_map(lambda x: x.permute(2, 0, 1, 3, 4), inputs)

    if _inner_fig5_enabled and layer_id is not None:
        num_mini_batches = inputs["XK"].shape[0]
        # ‚úÖ Use the FROZEN weights for all mini-batches
        inputs["W1_0"] = W1_frozen.expand(num_mini_batches, -1, -1, -1, -1)
        inputs["b1_0"] = b1_frozen.expand(num_mini_batches, -1, -1, -1, -1)
        inputs["W2_0"] = W2_frozen.expand(num_mini_batches, -1, -1, -1, -1)
        inputs["b2_0"] = b2_frozen.expand(num_mini_batches, -1, -1, -1, -1)
        ...
```

## What This Means for the 99% "Improvement"

The 99% improvement we saw is **WRONG** because:

```
Current calculation:
improvement = (l0_final - lafter_final) / l0_final
            = (l(W‚ÇÇ‚ÇÄ‚ÇÑ‚Çá; x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá) - l(W‚ÇÇ‚ÇÄ‚ÇÑ‚Çá_updated; x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá)) / l(W‚ÇÇ‚ÇÄ‚ÇÑ‚Çá; x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá)
            = (2.0 - 0.01) / 2.0
            = 99.5%

This is comparing:
- W‚ÇÇ‚ÇÄ‚ÇÑ‚Çá (weights BEFORE processing x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá)
- W‚ÇÇ‚ÇÄ‚ÇÑ‚Çá_updated (weights AFTER processing x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá)

Which is just measuring the immediate gradient step benefit!
```

**What we SHOULD be measuring:**

```
Correct calculation:
improvement = (l(W‚ÇÄ; x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá) - l(W‚ÇÇ‚ÇÄ‚ÇÑ‚Çá_updated; x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá)) / l(W‚ÇÄ; x‚ÇÇ‚ÇÄ‚ÇÑ‚Çá)

This compares:
- W‚ÇÄ (FROZEN initial weights)
- W‚ÇÇ‚ÇÄ‚ÇÑ‚Çá_updated (weights after processing 2047 tokens)

This tells us: "How much better is the adapted model vs the un-adapted model on token 2047?"
```

## The Real Question

**If W‚ÇÄ loss changes from 1.972 ‚Üí 2.005, what's actually changing?**

Since W‚ÇÄ should be frozen, the only thing that can change is the **input data** (X1, reconstruction_target).

But the input data is SUPPOSED to change - each token is different!

So the "frozen" W‚ÇÄ loss varying across positions is **EXPECTED** - it just means different tokens have different inherent difficulty.

**The bug is that we're not using frozen W‚ÇÄ at all - we're using W‚Çú!**

## Impact Analysis

### What Figure 5 is Currently Measuring

1. **"l0" (blue line)**: Actually l(W‚Çú‚Çã‚ÇÅ; x‚Çú) - loss with weights from previous token
2. **"lprev" (orange line)**: l(W‚Çú‚Çã‚ÇÅ; x‚Çú) - loss before gradient step
3. **"lafter" (green line)**: l(W‚Çú; x‚Çú) - loss after gradient step

**Wait... "l0" and "lprev" are THE SAME!**

That's why at position 0 they're identical:
```
l0 (frozen weights): 1.979173
lprev (current weights): 1.979173
```

At position 0, both use W‚ÇÄ because no updates have happened yet.

But then they diverge because:
- l0 (supposedly W‚ÇÄ) actually tracks W‚Çú‚Çã‚ÇÅ
- lprev explicitly uses W‚Çú‚Çã‚ÇÅ

So they should be identical at ALL positions!

**Let me check the actual plot to see if this is true...**

If l0 and lprev are nearly identical throughout, that confirms the bug.

### What Figure 5 SHOULD Be Measuring

1. **l0 (blue)**: l(W‚ÇÄ; x‚Çú) - how well do frozen initial weights predict each token?
   - Should vary based on token difficulty
   - Should NOT trend downward (W‚ÇÄ never changes!)

2. **lprev (orange)**: l(W‚Çú‚Çã‚ÇÅ; x‚Çú) - how well do accumulated updates predict current token?
   - Should trend downward as model adapts
   - Measures cumulative learning benefit

3. **lafter (green)**: l(W‚Çú; x‚Çú) - after one more gradient step
   - Should be below lprev (immediate gradient benefit)
   - Shows one-step improvement

**Gap between blue and orange = cumulative TTT learning**
**Gap between orange and green = single gradient step benefit**

## Next Steps

1. ‚úÖ **Confirm the bug** by checking if l0 ‚âà lprev in the actual plot
2. ‚ö†Ô∏è **Fix the bug** by freezing W‚ÇÄ before the scan loop
3. üî¨ **Re-run evaluation** to get correct Figure 5
4. üìä **Analyze true improvement** with correct frozen W‚ÇÄ

## Expected Results After Fix

With the fix, we should see:

- **l0 (blue)**: Relatively flat or slightly varying (different token difficulties)
- **lprev (orange)**: Trending downward (cumulative learning)
- **lafter (green)**: Below lprev, also trending down

**The gap between blue and orange will show the TRUE TTT benefit!**

Currently, the 99% "improvement" is mostly just showing that one gradient step helps a lot, not that cumulative TTT adaptation is working.

---

## Summary

**Bug**: W‚ÇÄ not actually frozen - using W‚Çú‚Çã‚ÇÅ instead
**Impact**: Figure 5 shows wrong metrics, 99% "improvement" is misleading
**Fix**: Clone and freeze W‚ÇÄ before scan loop
**Priority**: HIGH - this invalidates our current Figure 5 results
