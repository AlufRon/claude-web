import logging
import torch
import torch.nn.functional as F
from torch import nn
from torch.distributed._tensor import Replicate, Shard
from torch.distributed.device_mesh import DeviceMesh
from torch.distributed.tensor import distribute_tensor

from ...utils import (SequenceMetadata, full_tensor,
                     place_into, shard_tensor, to_local)
from ...config import TTTConfig as ModelConfig
from .linear_triton import TritonLinear
from .mlp_tk import TkMLP
from .ops import ttt_linear, ttt_mlp
from .utils import apply_rotary_emb, precompute_freqs_cis_3d, precompute_audio_rope_1d, apply_audio_rotary_emb

logger = logging.getLogger(__name__)


class TTTWrapper(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()

        self.model_dim = config.model_dim
        self.num_heads = config.num_heads
        self.rope_theta = config.rope_theta

        if config.ssm_layer == "ttt_linear":
            logger.info(f"[TTT] Initializing TTTLinear layer")
            self.ttt = TTTLinear(config, use_kernel=False)
        elif config.ssm_layer == "ttt_mlp":
            # Use multi-layer implementation if 3+ layers, else use standard 2-layer
            if hasattr(config, 'ttt_mlp_layers') and getattr(config, 'ttt_mlp_layers', 2) >= 3:
                num_layers = getattr(config, 'ttt_mlp_layers', 2)
                logger.info(f"[TTT] Using {num_layers}-layer TTT-MLP")
                self.ttt = TTTMLPMultiLayer(config, use_kernel=False)
            else:
                logger.info(f"[TTT] Using 2-layer TTT-MLP")
                self.ttt = TTTMLP(config, use_kernel=False)
        else:
            raise TypeError(f"No ttt layer of type {config.ssm_layer}")

    def _precompute_audio_rope_1d(self, seq_len: int) -> torch.Tensor:
        """Compute 1D Audio RoPE for given sequence length."""
        return precompute_audio_rope_1d(
            self.model_dim // self.num_heads,
            seq_len,
            self.rope_theta,
        )
    
    def reset_rope_position(self):
        """Manually reset RoPE position tracking (useful for inference/evaluation)."""
        if hasattr(self.ttt, 'stream_position'):
            self.ttt.stream_position = 0

    def reset_ttt_states(self):
        """Reset TTT inner weights to initial values."""
        if hasattr(self.ttt, 'init_weights'):
            # Handle different TTT implementations
            if hasattr(self.ttt, 'W1'):
                old_norm = self.ttt.W1.data.norm().item()
                self.ttt.init_weights()
                new_norm = self.ttt.W1.data.norm().item()
                logger.info(f"TTT weights reset: {old_norm:.6f} â†’ {new_norm:.6f}")
            elif hasattr(self.ttt, 'weights'):
                old_norm = self.ttt.weights[0].data.norm().item()
                self.ttt.init_weights()
                new_norm = self.ttt.weights[0].data.norm().item()
                logger.info(f"TTT weights reset: {old_norm:.6f} â†’ {new_norm:.6f}")
            else:
                self.ttt.init_weights()
                logger.info(f"TTT weights reset")

            self.reset_rope_position()
            return True
        else:
            logger.warning(f"{type(self.ttt).__name__} doesn't have init_weights method")
            return False

    def forward(self, x: torch.Tensor, seq_metadata: SequenceMetadata, layer_id: int = None, cache_params=None, is_inference: bool = False):
        seq_len = x.shape[1]  # B, L, D

        # Compute RoPE only if enabled in config
        if hasattr(self.ttt.config, 'use_rope') and self.ttt.config.use_rope:
            freqs_cis = self._precompute_audio_rope_1d(seq_len)
            freqs_cis = freqs_cis.to(x.device)
        else:
            freqs_cis = None

        # Define inputs by processing the hidden states
        inputs = self.ttt.process_input(x, freqs_cis, seq_metadata)
        
        # ==== SIMPLE WEIGHT PERSISTENCE CHECK ====
        # Only log when weights actually change
        if layer_id is not None and hasattr(self.ttt, '_last_weight_hash'):
            weight = self.ttt.weights[0] if hasattr(self.ttt, 'weights') else self.ttt.W1
            
            with torch.no_grad():
                current_hash = hash(weight.data.sum().item())
                
                if current_hash != self.ttt._last_weight_hash:
                    norm = weight.data.norm().item()
                    logger.info(f"âœ“ L{layer_id}: Weights CHANGED | norm={norm:.6f}")
                    self.ttt._last_weight_hash = current_hash
                    
        elif layer_id is not None:
            # First call - establish baseline
            weight = self.ttt.weights[0] if hasattr(self.ttt, 'weights') else self.ttt.W1
            with torch.no_grad():
                self.ttt._last_weight_hash = hash(weight.data.sum().item())
                norm = weight.data.norm().item()
                logger.info(f"ðŸ“Œ L{layer_id}: Baseline | norm={norm:.6f}")
        
        # Execute the core TTT logic
        XQW_batch = self.ttt.ttt(inputs, layer_id=layer_id)
        
        return XQW_batch


class TTTBase(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        self.width = config.model_dim
        self.num_heads = config.num_heads
        self.head_dim = config.model_dim // config.num_heads
        self.mini_batch_size = config.mini_batch_size

        self.ttt_base_lr = config.ttt_base_lr
        self.scan_checkpoint_group_size = config.scan_checkpoint_group_size

        # RoPE configuration
        self.rope_theta = config.rope_theta

        self.tp_mesh: None | DeviceMesh = None

        self._init_qkvo_proj()
        self._init_ttt_lr_gate()
        self._init_ttt_ln()

        self.post_norm = nn.LayerNorm(self.width, eps=1e-6)

    # We must reinitialize after meta initialization
    def init_weights(self):
        for linear in (self.wq, self.wk, self.wv):
            nn.init.normal_(linear.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.wo.weight, mean=0.0, std=0.02)

        self.post_norm.reset_parameters()
        nn.init.ones_(self.ttt_norm_weight.data)
        nn.init.zeros_(self.ttt_norm_bias)
        nn.init.normal_(self.learnable_ttt_lr_weight, mean=0.0, std=0.02)
        nn.init.zeros_(self.learnable_ttt_lr_bias)

    def _init_qkvo_proj(self):
        self.wq = nn.Linear(self.width, self.num_heads * self.head_dim, bias=True)
        self.wk = nn.Linear(self.width, self.num_heads * self.head_dim, bias=True)
        self.wv = nn.Linear(self.width, self.num_heads * self.head_dim, bias=True)
        self.wo = nn.Linear(self.width, self.num_heads * self.head_dim, bias=True)

    def _init_ttt_lr_gate(self):
        linear_weight_data = nn.Linear(self.width, 1, bias=True).weight.data
        self.learnable_ttt_lr_weight = nn.Parameter(
            torch.stack(
                [torch.normal(0, 0.02, size=linear_weight_data.shape) for _ in range(self.num_heads)],
                dim=0,
            )
        )

        linear_bias_data = nn.Linear(self.width, 1, bias=True).bias.data
        self.learnable_ttt_lr_bias = nn.Parameter(
            torch.stack(
                [torch.zeros_like(linear_bias_data) for _ in range(self.num_heads)],
                dim=0,
            )
        )

    def _init_ttt_ln(self):
        ln_weight_data = nn.LayerNorm(self.head_dim).weight.data
        self.ttt_norm_weight = nn.Parameter(torch.tile(ln_weight_data.unsqueeze(0), (self.num_heads, 1)))
        ln_bias_data = nn.LayerNorm(self.head_dim).bias.data
        self.ttt_norm_bias = nn.Parameter(torch.tile(ln_bias_data.unsqueeze(0), (self.num_heads, 1)))

    def init_device_mesh(self, tp_mesh: DeviceMesh):
        self.tp_mesh = tp_mesh

        self.ttt_norm_weight = nn.Parameter(distribute_tensor(self.ttt_norm_weight, tp_mesh, [Shard(0)]))
        self.ttt_norm_bias = nn.Parameter(distribute_tensor(self.ttt_norm_bias, tp_mesh, [Shard(0)]))

        self.learnable_ttt_lr_weight = nn.Parameter(
            distribute_tensor(self.learnable_ttt_lr_weight, tp_mesh, [Replicate()])
        )
        self.learnable_ttt_lr_bias = nn.Parameter(distribute_tensor(self.learnable_ttt_lr_bias, tp_mesh, [Replicate()]))

    def shard_inputs(self, inputs):
        assert self.tp_mesh is not None, "Tensor parallel mesh must be initialized before sharding inputs."

        for key in inputs:
            assert inputs[key].shape[1] == self.num_heads, "Sharding is only supported on the head dimension."
            inputs[key] = shard_tensor(inputs[key], self.tp_mesh, dim=1)

        return inputs

    @torch.compile
    def get_qkv_projections(self, hidden_states):
        XQ, XK, XV = (
            self.wq(hidden_states),
            self.wk(hidden_states),
            self.wv(hidden_states),
        )
        return XQ, XK, XV

    @torch.compile
    def get_eta(self, X):
        learnable_ttt_lr_weight = full_tensor(self.learnable_ttt_lr_weight)
        learnable_ttt_lr_bias = full_tensor(self.learnable_ttt_lr_bias)

        ttt_lr = torch.einsum("bnkc,hdc->bhnkd", X, learnable_ttt_lr_weight) + learnable_ttt_lr_bias.reshape(
            1, -1, 1, 1, 1
        )

        ttt_lr = F.sigmoid(ttt_lr)

        ttt_lr = ttt_lr.permute(0, 1, 2, 4, 3)
        return self.ttt_base_lr * ttt_lr / self.head_dim

    @torch.compile
    def interleave(self, x: torch.Tensor, seq_metadata: SequenceMetadata):
        init_offset, num_chunks, text_length = (
            seq_metadata.init_offset,
            seq_metadata.num_chunks,
            seq_metadata.text_length,
        )
        assert init_offset is not None, "Init offset must be provided for interleaving."

        seq_text_length = text_length * num_chunks

        B, H, NC, C, HD = x.shape
        x_flatten = x.reshape(B, H, NC * C, HD)

        x_text = x_flatten[:, :, :seq_text_length]
        x_video = x_flatten[:, :, seq_text_length:]

        x_text = torch.chunk(x_text, num_chunks, dim=2)

        video_init_offset = init_offset - text_length
        partial_chunks = torch.chunk(x_video[:, :, video_init_offset:], num_chunks - 1, dim=2)
        x_video = (x_video[:, :, :video_init_offset],) + partial_chunks

        x_interleaved = []
        for i in range(num_chunks):
            x_interleaved.append(torch.cat((x_text[i], x_video[i]), dim=2))

        return torch.cat(x_interleaved, dim=2).reshape(B, H, NC, C, HD)

    @torch.compile
    def undo_interleave(self, x: torch.Tensor, seq_metadata: SequenceMetadata):
        text_length, init_offset, base_offset, num_chunks = (
            seq_metadata.text_length,
            seq_metadata.init_offset,
            seq_metadata.base_offset,
            seq_metadata.num_chunks,
        )

        assert base_offset is not None, "Base offset must be provided for undoing interleaving."
        assert init_offset is not None, "Init offset must be provided for undoing interleaving."

        text_embs, vid_embs = torch.tensor([], dtype=x.dtype, device=x.device), torch.tensor(
            [], dtype=x.dtype, device=x.device
        )

        for i in range(num_chunks):
            if i == 0:
                scene_start_idx = 0
                scene_end_idx = init_offset
            else:
                scene_start_idx = init_offset + (i - 1) * base_offset
                scene_end_idx = init_offset + i * base_offset

            scene_emb = x[:, scene_start_idx:scene_end_idx]

            text_embs = torch.cat((text_embs, scene_emb[:, :text_length]), dim=1)
            vid_embs = torch.cat((vid_embs, scene_emb[:, text_length:]), dim=1)

        return torch.cat((text_embs, vid_embs), dim=1)

    def ln_reconstruction_target(self, XV, XK):
        XV = XV - XK
        eps = 1e-5

        mean = XV.mean(dim=-1, keepdim=True)
        std = place_into(to_local(XV).std(dim=-1, keepdim=True), XV)

        XV = (XV - mean) / (std + eps)
        XV = self.ttt_norm_weight.unsqueeze(0).unsqueeze(0) * XV + self.ttt_norm_bias.unsqueeze(0).unsqueeze(0)

        return XV + XK

    @torch.compile
    def reshape_to_mini_batch(self, X, XQ, XK, XV):
        B, L = X.shape[:2]
        num_mini_batch = L // self.mini_batch_size

        if not hasattr(self, '_reshape_logged'):
            self._reshape_logged = True
            logger.info(f"[TTT] mini_batch_size={self.mini_batch_size}, {num_mini_batch} SGD steps per forward")

        XQ, XK, XV = XQ.transpose(1, 2), XK.transpose(1, 2), XV.transpose(1, 2)

        X = X.reshape(B, num_mini_batch, self.mini_batch_size, self.width)

        XQ = XQ.reshape(B, self.num_heads, num_mini_batch, self.mini_batch_size, self.head_dim)
        XK = XK.reshape(B, self.num_heads, num_mini_batch, self.mini_batch_size, self.head_dim)
        XV = XV.reshape(B, self.num_heads, num_mini_batch, self.mini_batch_size, self.head_dim)

        return X, XQ, XK, XV

    def process_input(self, hidden_states: torch.Tensor, freqs_cis: torch.Tensor, seq_metadata: SequenceMetadata):
        B, L = hidden_states.shape[:2]
        mini_batch_size = self.mini_batch_size

        XQ, XK, XV = self.get_qkv_projections(hidden_states)

        XQ = XQ.view(B, L, -1, self.head_dim)
        XK = XK.view(B, L, -1, self.head_dim)
        XV = XV.view(B, L, -1, self.head_dim)

        # L2 Norm
        XQ = place_into(torch.nn.functional.normalize(to_local(XQ), p=2, dim=-1), XQ)
        XK = place_into(torch.nn.functional.normalize(to_local(XK), p=2, dim=-1), XK)

        # Apply 1D Audio RoPE conditionally
        if self.config.use_rope:
            positions = torch.arange(L, device=hidden_states.device, dtype=torch.long)
            positions_bounded = positions % mini_batch_size

            from .utils import precompute_audio_rope_1d
            freqs_cis_bounded = precompute_audio_rope_1d(
                self.head_dim,
                mini_batch_size,
                self.rope_theta,
                audio_scaling=True,
            ).to(hidden_states.device)

            freqs_cis = freqs_cis_bounded[positions_bounded]

            XQ_rope, XK_rope = apply_audio_rotary_emb(
                to_local(XQ), to_local(XK), freqs_cis=to_local(freqs_cis)
            )

            XQ = place_into(XQ_rope, XQ)
            XK = place_into(XK_rope, XK)

            if not hasattr(self, '_rope_logged'):
                self._rope_logged = True
                logger.info(f"[TTT] RoPE enabled with position modulo")

        XV = self.ln_reconstruction_target(XV, XK)

        hidden_states, XQ, XK, XV = self.reshape_to_mini_batch(hidden_states, XQ, XK, XV)

        ttt_lr_eta = self.get_eta(hidden_states)
        eta = 1 / mini_batch_size * ttt_lr_eta.repeat(1, 1, 1, mini_batch_size, 1)

        inputs = {
            "XQ": XQ,
            "XK": XK,
            "XV": XV,
            "eta": eta,
        }

        if self.tp_mesh is not None:
            inputs = self.shard_inputs(inputs)

        return inputs

    def ttt(
        self,
        inputs,
        layer_id: int = None,
        cache_params=None,
        is_inference: bool = False,
    ):
        raise NotImplementedError("ttt method must be implemented in TTTBase subclasses.")

    def forward(
        self,
        hidden_states: torch.Tensor,
        freqs_cis: torch.Tensor,
        seq_metadata: SequenceMetadata,
        layer_id: int = None,
        cache_params=None,
        is_inference: bool = False,
    ):
        assert (
            hidden_states.size(1) % self.config.mini_batch_size == 0
        ), "Sequence len must be multiple of mini batch size."

        hidden_states = self.ttt(
            self.process_input(hidden_states, freqs_cis, seq_metadata), 
            layer_id=layer_id,
            cache_params=cache_params,
            is_inference=is_inference,
        )

        hidden_states = self.post_norm(hidden_states)
        hidden_states = self.wo(hidden_states)

        hidden_states = full_tensor(hidden_states)

        if seq_metadata.is_multiscene:
            hidden_states = self.undo_interleave(to_local(hidden_states), seq_metadata)

        return hidden_states


class TTTLinear(TTTBase):
    def __init__(self, config: ModelConfig, use_kernel: bool = True):
        super().__init__(config)
        self.W1 = nn.Parameter(torch.normal(0, 0.02, size=(self.num_heads, self.head_dim, self.head_dim)))
        self.b1 = nn.Parameter(torch.zeros(self.num_heads, 1, self.head_dim))
        self.use_kernel = use_kernel

    def init_weights(self):
        super().init_weights()
        nn.init.normal_(self.W1, mean=0.0, std=0.02)
        nn.init.zeros_(self.b1)

    def init_device_mesh(self, tp_mesh: DeviceMesh):
        assert self.use_kernel, "Tensor parallel is not currently supported for TTTLinear without kernel."
        super().init_device_mesh(tp_mesh)

        self.W1 = nn.Parameter(distribute_tensor(self.W1, tp_mesh, [Shard(0)]))
        self.b1 = nn.Parameter(distribute_tensor(self.b1, tp_mesh, [Shard(0)]))

        TritonLinear.sharded_mode = True

    def ttt(self, inputs):
        B = inputs["XV"].shape[0]
        L = inputs["XV"].shape[2] * inputs["XV"].shape[3]
        num_mini_batch = inputs["XV"].shape[2]

        W1_states = torch.tile(self.W1.unsqueeze(0), dims=(B, 1, 1, 1))
        b1_states = torch.tile(self.b1.unsqueeze(0), dims=(B, 1, 1, 1))

        if not self.training:
            checkpoint_group_size = 0
        else:
            checkpoint_group_size = min(max(self.config.scan_checkpoint_group_size, 1), num_mini_batch)

        if self.use_kernel:
            XQW_batch = TritonLinear.apply(
                self.ttt_norm_weight,
                self.ttt_norm_bias,
                W1_states,
                b1_states,
                inputs["XQ"],
                inputs["XV"],
                inputs["XK"],
                inputs["eta"],
                checkpoint_group_size,
            )

            XQW_batch = XQW_batch.permute(0, 2, 3, 1, 4)
        else:
            XQW_batch = ttt_linear(
                inputs["XK"],
                inputs["XQ"],
                inputs["XV"],
                inputs["eta"],
                self.ttt_norm_weight,
                self.ttt_norm_bias,
                W1_states,
                b1_states,
                checkpoint_group_size,
            )

        XQW_batch = XQW_batch.reshape(B, L, self.width)
        return XQW_batch


class TTTMLPMultiLayer(TTTBase):
    """Multi-layer TTT-MLP implementation for 3+ layers"""
    
    def __init__(self, config: ModelConfig, use_kernel: bool = False):
        super().__init__(config)
        
        self.num_layers = getattr(config, 'ttt_mlp_layers', 2)
        self.expansion_factor = getattr(config, 'ttt_mlp_expansion_factor', 4.0)
        self.custom_dims = getattr(config, 'ttt_mlp_hidden_dims', None)
        
        assert self.num_layers >= 3, f"TTTMLPMultiLayer requires 3+ layers, got {self.num_layers}"
        
        self.layer_dims = self._calculate_layer_dimensions()
        
        self.weights = nn.ParameterList()
        self.biases = nn.ParameterList()
        
        for i in range(self.num_layers):
            in_dim = self.layer_dims[i]
            out_dim = self.layer_dims[i + 1]
            
            weight = nn.Parameter(torch.normal(0, 0.02, size=(self.num_heads, in_dim, out_dim)))
            bias = nn.Parameter(torch.zeros(self.num_heads, 1, out_dim))
            
            self.weights.append(weight)
            self.biases.append(bias)
        
        logger.info(f"[TTT-MLP-MULTI] {self.num_layers} layers: {self.layer_dims}")

        self.use_kernel = False
        self.stream_position = 0
        
    def _calculate_layer_dimensions(self):
        """Calculate the input/output dimensions for each layer"""
        if self.custom_dims is not None:
            dims = [self.head_dim] + self.custom_dims + [self.head_dim]
        else:
            expanded_dim = int(self.head_dim * self.expansion_factor)
            dims = [self.head_dim]
            for _ in range(self.num_layers - 1):
                dims.append(expanded_dim)
            dims.append(self.head_dim)
        
        return dims
        
    def init_weights(self):
        super().init_weights()
        for weight, bias in zip(self.weights, self.biases):
            nn.init.normal_(weight, mean=0.0, std=0.02)
            nn.init.zeros_(bias)
    
    def init_device_mesh(self, tp_mesh: DeviceMesh):
        raise NotImplementedError("Tensor parallelism not yet supported for TTTMLPMultiLayer")
    
    def ttt(self, inputs, layer_id=None, cache_params=None, is_inference: bool = False):
        B = inputs["XV"].shape[0]
        num_mini_batch = inputs["XV"].shape[2]
        L = inputs["XV"].shape[2] * inputs["XV"].shape[3]

        if cache_params is not None and is_inference:
            logger.warning("[TTTMLPMultiLayer] Cache-based inference not implemented, falling back")
            cache_params = None

        weight_states = []
        bias_states = []
        
        for weight, bias in zip(self.weights, self.biases):
            weight_states.append(torch.tile(weight.unsqueeze(0), dims=(B, 1, 1, 1)))
            bias_states.append(torch.tile(bias.unsqueeze(0), dims=(B, 1, 1, 1)))
        
        if not self.training:
            checkpoint_group_size = 0
        else:
            checkpoint_group_size = min(max(self.config.scan_checkpoint_group_size, 1), num_mini_batch)
        
        from .ops.ttt_mlp import ttt_mlp_multi_layer
        
        result = ttt_mlp_multi_layer(
            inputs["XK"],
            inputs["XQ"],
            inputs["XV"],
            inputs["eta"],
            self.ttt_norm_weight,
            self.ttt_norm_bias,
            weight_states,
            bias_states,
            checkpoint_group_size,
            log_losses=False,
            layer_id=layer_id,
            stream_pos_base=self.stream_position,
            return_updated_weights=not self.training,
        )
        
        # Handle result
        if isinstance(result, tuple) and len(result) == 3:
            XQW_batch, updated_weight_states, updated_bias_states = result
            
            # Write back updated weights during inference
            if not self.training and updated_weight_states is not None:
                with torch.no_grad():
                    # MINIMAL LOG: Track weight changes every 10 tokens to detect oscillation
                    if hasattr(self, '_weight_update_count'):
                        self._weight_update_count += 1
                    else:
                        self._weight_update_count = 0

                    # Sample first weight before update for oscillation detection
                    if self._weight_update_count % 10 == 0:
                        old_norm = self.weights[0].data.norm().item()

                    for i, (updated_w, updated_b) in enumerate(zip(updated_weight_states, updated_bias_states)):
                        self.weights[i].data.copy_(updated_w[0])
                        self.biases[i].data.copy_(updated_b[0])

                    # Log every 100 updates to avoid spam
                    if self._weight_update_count % 100 == 0:
                        new_norm = self.weights[0].data.norm().item()
                        change = abs(new_norm - old_norm)
                        if change < 0.001:  # Warn if weights barely changing (stricter threshold)
                            logger.warning(f"âš ï¸  [TTT-WEIGHT-STUCK] Token={self._weight_update_count} | Weight_norm={new_norm:.6f} | Change={change:.6f} (very small)")
                        elif self._weight_update_count > 0:
                            logger.info(f"âœ“ [TTT-WEIGHT-UPDATE] Token={self._weight_update_count} | Old={old_norm:.6f} â†’ New={new_norm:.6f} | Î”={change:.6f}")
        else:
            XQW_batch = result

        self.stream_position += num_mini_batch

        XQW_batch = XQW_batch.reshape(B, L, self.width)
        return XQW_batch


class TTTMLP(TTTBase):
    def __init__(self, config: ModelConfig, use_kernel: bool = True):
        super().__init__(config)
        self.W1 = nn.Parameter(torch.normal(0, 0.02, size=(self.num_heads, self.head_dim, 4 * self.head_dim)))
        self.b1 = nn.Parameter(torch.zeros(self.num_heads, 1, 4 * self.head_dim))
        self.W2 = nn.Parameter(torch.normal(0, 0.02, size=(self.num_heads, 4 * self.head_dim, self.head_dim)))
        self.b2 = nn.Parameter(torch.zeros(self.num_heads, 1, self.head_dim))

        self.use_kernel = use_kernel
        self.stream_position = 0

    def init_weights(self):
        super().init_weights()
        nn.init.normal_(self.W1, mean=0.0, std=0.02)
        nn.init.zeros_(self.b1)
        nn.init.normal_(self.W2, mean=0.0, std=0.02)
        nn.init.zeros_(self.b2)

    def init_device_mesh(self, tp_mesh: DeviceMesh):
        assert self.use_kernel, "Tensor parallel is not currently supported for TTTMLP without kernel."
        super().init_device_mesh(tp_mesh)

        self.W1 = nn.Parameter(distribute_tensor(self.W1, tp_mesh, [Shard(0)]))
        self.b1 = nn.Parameter(distribute_tensor(self.b1, tp_mesh, [Shard(0)]))
        self.W2 = nn.Parameter(distribute_tensor(self.W2, tp_mesh, [Shard(0)]))
        self.b2 = nn.Parameter(distribute_tensor(self.b2, tp_mesh, [Shard(0)]))

        TkMLP.sharded_mode = True

    def ttt(self, inputs, layer_id=None, cache_params=None, is_inference: bool = False):
        B = inputs["XV"].shape[0]
        num_mini_batch = inputs["XV"].shape[2]
        L = inputs["XV"].shape[2] * inputs["XV"].shape[3]

        W1_states = torch.tile(self.W1.unsqueeze(0), dims=(B, 1, 1, 1))
        b1_states = torch.tile(self.b1.unsqueeze(0), dims=(B, 1, 1, 1))
        W2_states = torch.tile(self.W2.unsqueeze(0), dims=(B, 1, 1, 1))
        b2_states = torch.tile(self.b2.unsqueeze(0), dims=(B, 1, 1, 1))

        if not self.training:
            checkpoint_group_size = 0
        else:
            checkpoint_group_size = min(max(self.config.scan_checkpoint_group_size, 1), num_mini_batch)

        if self.use_kernel:
            XQW_batch = TkMLP.apply(
                self.ttt_norm_weight,
                self.ttt_norm_bias,
                W1_states,
                b1_states,
                W2_states,
                b2_states,
                inputs["XQ"],
                inputs["XV"],
                inputs["XK"],
                inputs["eta"],
                checkpoint_group_size,
            )

            XQW_batch = XQW_batch.permute(0, 2, 3, 1, 4)
        else:
            XQW_batch = ttt_mlp(
                inputs["XK"],
                inputs["XQ"],
                inputs["XV"],
                inputs["eta"],
                self.ttt_norm_weight,
                self.ttt_norm_bias,
                W1_states,
                b1_states,
                W2_states,
                b2_states,
                checkpoint_group_size,
                log_losses=False,
                layer_id=layer_id,
                stream_pos_base=self.stream_position,
            )

            self.stream_position += num_mini_batch

        XQW_batch = XQW_batch.reshape(B, L, self.width)
        return XQW_batch