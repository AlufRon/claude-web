# data
data:
  train_data: /sise/eliyanac-group/ron_al/daily-talk-contiguous/train/dailytalk_train.jsonl
  eval_data: /sise/eliyanac-group/ron_al/daily-talk-contiguous/eval/dailytalk_eval.jsonl
  shuffle: true

ttt:
  enable: false  # No TTT - frozen baseline

# model
moshi_paths: 
  hf_repo_id: "kyutai/moshiko-pytorch-bf16"

full_finetuning: false # No fine-tuning at all
lora:
  enable: false # No LoRA - completely frozen

paper_metrics:
  paper_metrics_eval: true
  paper_metrics_freq: 10  # Evaluate more frequently since no training
  # OPTIMAL CONFIGURATION: Natural silence codes for +2-6% performance boost
  paper_metrics_use_silence: true
  # Cross-stream evaluation (experimental - for architecture research)
  paper_metrics_use_user_stream: false
  sblimp_audio_dir: /sise/eliyanac-group/ron_al/sblimp_data/sLM21_dataset/syntactic/test/
  sblimp_gold_csv: /sise/eliyanac-group/ron_al/sblimp_data/sLM21_dataset/syntactic/test/gold.csv
  sblimp_max_pairs: 2000  # Evaluation sample size  
  sstory_audio_dir: /sise/eliyanac-group/ron_al/sSC/sSC/
  sstory_max_pairs: 2000
  swuggy_audio_dir: /sise/eliyanac-group/ron_al/sblimp_data/sLM21_dataset/lexical/test/
  swuggy_gold_csv: /sise/eliyanac-group/ron_al/sblimp_data/sLM21_dataset/lexical/test/gold.csv
  swuggy_max_pairs: 2000
  tstory_audio_dir: /sise/eliyanac-group/ron_al/tSC/tSC/
  tstory_max_pairs: 2000
  # LibriLight Long Context Evaluation (TTT Paper methodology)
  librilight_audio_dir: /sise/eliyanac-group/ron_al/librilight/extracted_medium/medium/
  librilight_evaluation_mode: single_book  # Options: single_book, multi_book, random
  librilight_speaker_id: "100"  # Specific speaker for single_book mode
  librilight_book_name: "emerald_city_librivox_64kb_mp3"  # Specific book for single_book mode
  librilight_max_chapters: 3  # Number of chapters to concatenate per sequence
  librilight_num_sequences: 1  # Number of different sequences for multi_book mode


first_codebook_weight_multiplier: 100.
text_padding_weight: .5

# optim - minimal since no training
duration_sec: 15
batch_size: 1
max_steps: 1000  # Enough steps to trigger evaluation
gradient_checkpointing: false  # Not needed for frozen model
optim:
  lr: 1e-10  # Minimal learning rate to enable training loop
  weight_decay: 0.0
  pct_start: 0.0

# other
seed: 123
log_freq: 10
eval_freq: 10  # Evaluate every 10 steps
do_eval: true
do_ckpt: false  # No checkpointing needed
ckpt_freq: 1000

save_adapters: false # No adapters to save

run_dir: /sise/eliyanac-group/ron_al/frozen_baseline_shuffled_1000steps
 # True frozen baseline

wandb:
  project: ttt-moshi-production
  offline: false
  run_name: frozen_baseline_shuffled_1000steps