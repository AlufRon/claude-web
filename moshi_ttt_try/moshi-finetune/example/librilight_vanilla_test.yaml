# LibriLight Vanilla Test - NO TTT, NO LoRA
# Purpose: Test if NaN issue is fixed with vanilla model
# Very minimal to avoid OOM

run_dir: /sise/eliyanac-group/ron_al/librilight_vanilla_test

# Data configuration
data:
  train_data: /sise/eliyanac-group/ron_al/librilight/extracted_medium/medium/librilight.jsonl
  eval_data: ""
  shuffle: false

duration_sec: 5  # Very short

# Model paths
moshi_paths:
  hf_repo_id: kyutai/moshiko-pytorch-bf16

# Training hyperparameters - MINIMAL
batch_size: 1
num_microbatches: 1
max_steps: 3  # Just 3 steps to see loss values
log_freq: 1
ckpt_freq: 10
num_ckpt_keep: 1
do_ckpt: false  # Don't checkpoint
do_eval: false
seed: 42

# Text weighting
first_codebook_weight_multiplier: 1.0
text_padding_weight: 0.001

# Optimizer
optim:
  lr: 1e-5  # Very small LR
  weight_decay: 0.1
  pct_start: 0.05

# LoRA - DISABLED
lora:
  enable: false
  rank: 32
  scaling: 2.0
  ft_embed: false

full_finetuning: false
save_adapters: true

# TTT - DISABLED
ttt:
  enable: false
  layers: "31"
  base_lr: 1.0
  mini_batch_size: 8
  persistent_states: true
  initial_gating_alpha: 0.1
  ttt_mlp_layers: 2
  ttt_mlp_expansion_factor: 2.0

# Efficiency
gradient_checkpointing: false  # DISABLED to avoid checkpointing error
max_norm: 1.0

# Wandb
wandb:
  project: null

resume_from: null
reset_optimizer: false
reset_step: false

param_dtype: bfloat16
overwrite_run_dir: true
