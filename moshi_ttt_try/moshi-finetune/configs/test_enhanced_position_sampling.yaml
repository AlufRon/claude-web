# Test configuration for enhanced position sampling
# Minimal settings to test the new position-by-position evaluation

# Data configuration
data:
  train_data: /sise/eliyanac-group/ron_al/daily-talk-contiguous/train/dailytalk_train.jsonl
  eval_data: /sise/eliyanac-group/ron_al/daily-talk-contiguous/eval/dailytalk_eval.jsonl
  shuffle: true

# Training run configuration
run_dir: runs/test_enhanced_position_sampling
overwrite_run_dir: true

# Model paths
moshi_paths:
  hf_repo_id: kyutai/moshiko-pytorch-bf16

# LoRA configuration (for efficiency)
lora:
  enable: true
  rank: 32
  scaling: 1.0
  ft_embed: false

# TTT disabled for baseline test
ttt:
  enable: false

# Training parameters (minimal for quick test)
full_finetuning: false
duration_sec: 10.0      # Short audio segments
batch_size: 1           # Single batch
num_microbatches: 1     # No gradient accumulation
max_steps: 2            # Just 2 steps for validation
log_freq: 1             # Log every step

# Loss weighting
first_codebook_weight_multiplier: 1.0
text_padding_weight: 0.5

# Optimizer configuration  
optim:
  lr: 1e-4
  weight_decay: 0.01
  pct_start: 0.1

# Regularization
max_norm: 1.0

# Enable evaluation with paper metrics
do_eval: true
eval_freq: 1  # Evaluate every step

# Paper metrics configuration - ENABLE for testing
paper_metrics:
  paper_metrics_eval: true
  paper_metrics_freq: 1  # Run every step for testing
  librilight_audio_dir: /sise/eliyanac-group/ron_al/librilight/extracted_medium/medium/
  librilight_evaluation_mode: single_book
  librilight_max_chapters: 1
  librilight_num_sequences: 1

# Checkpointing (disabled for quick test)
do_ckpt: false

# System configuration
seed: 42
gradient_checkpointing: false
param_dtype: float32

# Monitoring - disable wandb for quick test
wandb:
  project: null